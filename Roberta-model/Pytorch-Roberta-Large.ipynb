{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can run the Notebook Pytorch-Roberta_Large.ipynb either Locally or in Kaggle - Just modify the 'ROOT_DIR' variable to properly refer to the dataset\n",
    "Feedback Prize-Evaluating Student Writing\n",
    "\n",
    "### The classification Problem in this competition\n",
    "Basically, we have a bunch of essays written by kids in the age range of about 12-18 years old in which we have to find word sequences that can be classified as one of 7 \"discourse types\", which are\n",
    "\n",
    "* Lead - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the readerâ€™s attention and point toward the thesis Interestingly, this is not present in all the the training set data. Around 40% of the ids do not have a lead!\n",
    "\n",
    "* Position - an opinion or conclusion on the main question Almost all the training data has a Position. However, it is not always at the beginning.\n",
    "\n",
    "* Claim - a claim that supports the position Almost all the training data has a Claim. One training set data has 12 Claims!! Over half have at least 3 claims\n",
    "\n",
    "* Counterclaim - a claim that refutes another claim or gives an opposing reason to the position\n",
    "\n",
    "* Rebuttal - a claim that refutes a counterclaim\n",
    "\n",
    "* Evidence - ideas or examples that support claims, counterclaims, or rebuttals.\n",
    "\n",
    "* Concluding Statement - a concluding statement that restates the claims\n",
    "\n",
    "So basically, we are tasked with giving feedback on argumentative essays written by students. Specifically, our task is to predict the human annotations.\n",
    "\n",
    "This annotation will be done in 2 steps:\n",
    "\n",
    "- Segment each essay into discrete, rhetorical and argumentative elements (i.e., discourse elements) .\n",
    "- Classify each element as one of 7 \"discourse types\",\n",
    "### Basics on the training data\n",
    "We have a bunch of text files and a separate train.csv with labels.\n",
    "\n",
    "In the csv, we get reference to the text files, and then multiple lines per text file with spans indicating a specific discourse type.\n",
    "\n",
    "The Ground Truth here is a combination of the discourse type and the prediction string.\n",
    "\n",
    "Here, Kaggle provides labels as characters (discourse start, discourse end) or words (predictionstring).\n",
    "\n",
    "train.csv - a .csv file containing the annotated version of all essays in the training set\n",
    "\n",
    "* discourse_id - discourse_id is a unique identifier for each discourse element. Basically, each row in train.csv contains one discourse element, and the discourse_id is what identifies each of these discourse elements.\n",
    "It is different to the 'id' column in that 'id' is the identifier of the full student essay that the discourse element came from. So multiple rows can have the same value for 'id'. On the other hand, each row in train.csv will have a different value for 'discourse_id', since each row is a different discourse element.\n",
    "\n",
    "Hope that helps and let me know if it's still unclear.\n",
    "\n",
    "* discourse_start - character position where discourse element begins in the essay response\n",
    "\n",
    "* discourse_end - character position where discourse element ends in the essay response\n",
    "\n",
    "* discourse_text - text of discourse element\n",
    "\n",
    "* discourse_type - classification of discourse element\n",
    "\n",
    "* discourse_type_num - enumerated class label of discourse element\n",
    "\n",
    "* predictionstring - the word indices of the training sample, as required for predictions\n",
    "\n",
    "The **predictionstring** corresponds to - the index of the words in the essay and the predicted discourse type for this sequence of words, SHOULD be correct.\n",
    "\n",
    "There can be partial matches, if the correct discourse type is predicted but on a longer or shorter sequence of words than specified in the Ground Truth.\n",
    "\n",
    "https://www.kaggle.com/code/erikbruin/nlp-on-student-writing-eda\n",
    "\n",
    "### Notebook keypoints\n",
    "* Build a baseline model taking this as a token classification or \"NER\" problem\n",
    "* Build a \"RoBERTa-large\" model with max_length=512\n",
    "* A NER \"ShortFormer\" with chunks, strides\n",
    "* Manage the chunking with stride of the texts with length greater than 512 (and the posterior merge).\n",
    "1. At the tokenizing step, where I used the hugging face tokenizer functionality to leverage the chunking\n",
    "2. Validation is now performed on a per-epoch fashion\n",
    "### The reason behind the focus on Longformer in this competiton\n",
    "For the Transformer models, the Transformer architecture - has a matrix multiplication that scales quadratically with the input sequence length in terms of memory, making the regular Transformer very expensive for longer sequences. Because of which we have the 512 tokens max length in the BERT-like models.\n",
    "\n",
    "As a partial solution to this, we have LongFormer and BigBird\n",
    "\n",
    "The Longformer model's attention mechanism is designed to enable efficient processing of long sequences. It differs from standard self-attention mechanisms in a few key ways:\n",
    "\n",
    "Sliding Window Attention: Instead of attending to all positions in the input sequence as in standard self-attention, the Longformer attends only to a sliding window of adjacent positions. This reduces the computational complexity of the attention mechanism from O(n^2) to O(n), where n is the length of the input sequence.\n",
    "\n",
    "With LongFormer the lower cost of sparse self-attention mechanisms allows these models to handle up to 4096 in a regular GPU, which is, 8x what a normal Transformer can.\n",
    "\n",
    "Given the lengths of the texts in this competition, many focus \"Longformer\" type models.\n",
    "\n",
    "However, the classic 512-token models such as BERT do have a mechanism to handle sequence of length greater than 512 basically by\n",
    "\n",
    "1. Split it into chunks of 512 tokens (with some overlap with the stride param)\n",
    "2. Use the model to process those chunks\n",
    "3. Merge back the predictions over the chunks to obtain predictions over the full text\n",
    "### Evaluation Metric - Overlap Concept in this competition\n",
    "#### 1. As per the Evaluation page - \"If the overlap between the ground truth and prediction is >= 0.5, and the overlap between the prediction and the ground truth >= 0.5, the prediction is a match and considered a true positive. If multiple matches exist, the match with the highest pair of overlaps is taken.\"\n",
    "The **\"overlap between the ground truth and prediction\"** is a measure of how many words in the ground truth match the predicted words. It is calculated as the number of words that appear in both the ground truth and the prediction divided by the total number of words in the ground truth.\n",
    "\n",
    "For example, if the ground truth for a sample contains 10 words and the model predicts 8 of those words correctly, the overlap between the ground truth and prediction would be 0.8 or 80%.\n",
    "\n",
    "On the other hand, the **\"overlap between the prediction and the ground truth\"** is a measure of how many words in the prediction match the words in the ground truth. It is calculated as the number of words that appear in both the ground truth and the prediction divided by the total number of words in the prediction.\n",
    "\n",
    "For example, if the model predicts 12 words for a sample and 10 of those words match the words in the ground truth, the overlap between the prediction and ground truth would be 0.83 or 83%.\n",
    "\n",
    "Example: Ground truth: 1 2 3 4 5 Prediction: 1 2 3\n",
    "\n",
    "Common indices: 1 2 3 (3 common indices)\n",
    "\n",
    "* overlap between the ground truth and prediction: 3 common indices / 5 ground truth indices = 0.6 (>= 0.5)\n",
    "\n",
    "* overlap between the prediction and the ground truth: 3 common indices / 3 prediction indices = 1.0 (>= 0.5)\n",
    "\n",
    "Since both overlaps are greater than or equal to 0.5, this prediction is considered a match (true positive).\n",
    "\n",
    "### 2. In both, the numerator is the size of the intersection. Both calculations have to be at least 0.5 for the prediction to be considered a hit.\n",
    "These two measures are used together to determine whether a prediction is a true positive or not.\n",
    "\n",
    "By using both measures, the evaluation metric ensures that the predicted words are not only present in the ground truth, but also that the ground truth words are correctly identified by the model.\n",
    "\n",
    "In some cases, there might be multiple matches (multiple predictions with an overlap >= 0.5). If this happens, the competition rules state that the match with the highest pair of overlaps is considered.\n",
    "\n",
    "### 3. \"If multiple matches exist, the match with the highest pair of overlaps is taken\" -\n",
    "This statement means that if there are several predictions that match a single ground truth, only the prediction with the highest overlaps (for both ground truth and prediction) will be considered a true positive.\n",
    "\n",
    "Example: Ground truth: 1 2 3 4 5 Prediction 1: 1 2 3 Prediction 2: 1 2 3 4\n",
    "\n",
    "In this example, both predictions match the ground truth. However, Prediction 2 has a higher overlap and will be considered the true positive. Prediction 1 will not be considered.\n",
    "\n",
    "### 4. \"Any unmatched ground truths are false negatives and any unmatched predictions are false positives.\"\n",
    "This statement means that any ground truth without a corresponding matching prediction is considered a false negative (missed correct prediction), and any prediction without a corresponding matching ground truth is considered a false positive (incorrect prediction).\n",
    "\n",
    "Example: Ground truth: 1 2 3 4 5 Prediction: 1 2\n",
    "\n",
    "Since the overlaps are not >= 0.5 for both ground truth and prediction, this prediction is considered a false positive. And since there's no match for the ground truth, it's considered a false negative.\n",
    "\n",
    "A basic implementation of the above concept will like below\n",
    "\n",
    "### Below code is just for clarification of the concept, as for implementations for this project, I will use a slightly different version of this code later.\n",
    "def calculate_overlap(ground_truth, prediction):\n",
    "    ground_truth_set = set(ground_truth)\n",
    "    prediction_set = set(prediction)\n",
    "    \n",
    "    intersection = ground_truth_set.intersection(prediction_set)\n",
    "    overlap_ground_truth = len(intersection) / len(ground_truth_set)\n",
    "    overlap_prediction = len(intersection) / len(prediction_set)\n",
    "    \n",
    "    return overlap_ground_truth, overlap_prediction\n",
    "\n",
    "def evaluate_overlap(ground_truth, prediction):\n",
    "    overlap_ground_truth, overlap_prediction = calculate_overlap(ground_truth, prediction)\n",
    "    \n",
    "    if overlap_ground_truth >= 0.5 and overlap_prediction >= 0.5:\n",
    "        return \"True Positive\"\n",
    "    elif overlap_ground_truth < 0.5:\n",
    "        return \"False Negative\"\n",
    "    elif overlap_prediction < 0.5:\n",
    "        return \"False Positive\"\n",
    "\n",
    "#Example usage\n",
    "ground_truth = [1, 2, 3, 4, 5]\n",
    "prediction = [1, 2, 3]\n",
    "\n",
    "result = evaluate_overlap(ground_truth, prediction)\n",
    "print(result)  # Output: True Positive\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import ast\n",
    "import time\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "from train import *\n",
    "from utils import *\n",
    "from const import *\n",
    "from validation import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_wandb(project='ner', run_name=RUN_NAME, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_org = pd.read_csv('../input/feedback-prize-2021/train.csv')\n",
    "df_train_org = pd.read_csv(ROOT_DIR + 'train.csv')\n",
    "print(df_train_org.shape)\n",
    "display(df_train_org.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new Column for plotting Length and frequency and relative position per discourse_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train_org_copy = df_train_org.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_org_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_org_copy[\"discourse_len\"] = df_train_org_copy[\"discourse_text\"].apply(lambda x: len(x.split()))\n",
    "df_train_org_copy[\"pred_len\"] = df_train_org_copy[\"predictionstring\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "cols_to_display = ['discourse_id', 'discourse_text', 'discourse_type','predictionstring', 'discourse_len', 'pred_len']\n",
    "\n",
    "df_train_org_copy[cols_to_display].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1 = df_train_org_copy.groupby('discourse_type')['discourse_len'].mean().sort_values().plot(kind=\"barh\")\n",
    "ax1.set_title(\"Average number of words versus Discourse Type\", fontsize=14, fontweight = 'bold')\n",
    "ax1.set_xlabel(\"Average number of words\", fontsize = 10)\n",
    "ax1.set_ylabel(\"\")\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2 = df_train_org_copy.groupby('discourse_type')['discourse_type'].count().sort_values().plot(kind=\"barh\")\n",
    "ax2.get_xaxis().set_major_formatter(FuncFormatter(lambda x, p: format(int(x), ','))) #add thousands separator\n",
    "ax2.set_title(\"Frequency of Discourse Type in all essays\", fontsize=14, fontweight = 'bold')\n",
    "ax2.set_xlabel(\"Frequency\", fontsize = 10)\n",
    "ax2.set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above graph, is there a correlation between the length of a discourse and the class (discourse_type)? Yes, there is. Evidence is the longest discourse type on average.\n",
    "\n",
    "When looking at the frequencies of occurence, we see that Counterclaim and Rebuttal are relatively rare\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Convert Train Text to NER Labels\n",
    "We will now convert all text words into NER labels and save in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = ROOT_DIR + 'train/'\n",
    "output_file = 'train_NER.csv'\n",
    "\n",
    "df_ner_texts = attach_ner_to_text(input_dir, output_file, df_train_org)\n",
    "df_ner_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have created one entity/label for each word correctly\n",
    "assert (df_ner_texts['text_split'].str.len() == df_ner_texts['entities'].str.len()).all(), \"Lengths of text_split and entities do not match for all rows.\"\n",
    "\n",
    "# df_ner_texts['text_split'].str.len(): computes the length (number of words) for each list in the 'text_split' column. The .str accessor is used to perform vectorized string operations, and the .len() method computes the length of each list.\n",
    "# The .all() method checks if all the elements in the resulting boolean Series are True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDS_ALL_UNIQUE = df_train_org.id.unique()\n",
    "print(f'There are {len(IDS_ALL_UNIQUE)} train texts. We will split 90% 10% for validation.')\n",
    "\n",
    "# train valid split 90% 10%\n",
    "\n",
    "# Set the random seed for NumPy to ensure that the random sampling will produce the same results each time the code is run. This is useful for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "train_idx = np.random.choice(np.arange(len(IDS_ALL_UNIQUE)),int(0.9*len(IDS_ALL_UNIQUE)),replace=False)\n",
    "#  np.arange() function generates an array of evenly spaced values. In this case, it creates an array of integers starting from 0 up to (but not including) len(IDS_ALL_UNIQUE). This array represents the indices of the unique IDs.\n",
    "# np.random.choice generates a random sample from a given 1-D array or range of integers. It can be used to select random elements from an array or to create random samples without replacement. The 2-nd param to np.random.choice() is the output size. replace=False, means each element can only be selected once. \n",
    "\n",
    "\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS_ALL_UNIQUE)),train_idx)\n",
    "# np.setdiff1d() function is used to find the difference between two arrays. In this case, it computes the difference between the array of all indices (np.arange(len(IDS))) and the selected training indices (train_idx). The result is an array containing the indices not present in the training set, which will be used for the validation set.\n",
    "np.random.seed(None)\n",
    "\n",
    "# CREATE TRAIN SUBSET AND VALID SUBSET\n",
    "df_train = df_ner_texts.loc[df_ner_texts['id'].isin(IDS_ALL_UNIQUE[train_idx])].reset_index(drop=True)\n",
    "\n",
    "df_val = df_ner_texts.loc[df_ner_texts['id'].isin(IDS_ALL_UNIQUE[valid_idx])].reset_index(drop=True)\n",
    "\n",
    "\"\"\" The `reset_index(drop=True)` call is used to reset the index of the DataFrame after filtering out some columns. By default, when columns are filtered from a DataFrame, the index remains the same, and the original index values are preserved. However, in some cases, such as when joining or merging DataFrames, it can be helpful to have a simple sequential index with no missing values.\n",
    "\n",
    "By setting `drop=True`, the old index is dropped and a new sequential index is created. The `copy()` method is used to create a copy of the DataFrame with the new index, instead of modifying the original DataFrame in place. \"\"\"\n",
    "\n",
    "print(f\"FULL Dataset : {df_ner_texts.shape}\")\n",
    "print(f\"TRAIN Dataset: {df_train.shape}\")\n",
    "print(f\"TEST Dataset : {df_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model from huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_hf_model(MODEL_PATH, MODEL_NAME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenized_train = tokenize(df_train, tokenizer, DOC_STRIDE, MAX_LEN, LABELS_TO_IDS)\n",
    "tokenized_val = tokenize(df_val, tokenizer, DOC_STRIDE, MAX_LEN, LABELS_TO_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_train['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train['overflow_to_sample_mapping'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderClass(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset class for tokenized data.\n",
    "\n",
    "    Args:\n",
    "        tokenized_ds (dict): Tokenized dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_ds):\n",
    "        self.data = tokenized_ds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves an item from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the item data.\n",
    "\n",
    "        \"\"\"\n",
    "        # print('print data ', self.data)\n",
    "        item = {k: self.data[k][index] for k in self.data.keys()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of items in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of items in the dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        return len(self.data['input_ids'])\n",
    "    \n",
    "\n",
    "# Create Datasets and DataLoaders for training and validation dat\n",
    "\n",
    "ds_train = DataLoaderClass(tokenized_train)\n",
    "\n",
    "ds_val = DataLoaderClass(tokenized_val)\n",
    "\n",
    "dataloader_train = DataLoader(ds_train, batch_size=config['train_batch_size'], \n",
    "                      shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "dataloader_val = DataLoader(ds_val, batch_size=config['valid_batch_size'], \n",
    "                    shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDS_TO_LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = AutoConfig.from_pretrained(MODEL_PATH+'/config.json') \n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "                   MODEL_PATH+'/pytorch_model.bin',config=config_model)\n",
    "model.to(config['device']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_valid_id_list = IDS_ALL_UNIQUE[valid_idx] # this variable needs to be passed as an argument to the validate() emthod\n",
    "\n",
    "# Below line is just for checking out that df_valid looks ok, as it will be used \n",
    "# inside the validate() method\n",
    "df_valid = df_train_org.loc[df_train_org['id'].isin(unique_valid_id_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here just testing the get_prediction() method\n",
    "out_of_fold = get_predictions(df_val, dataloader_val, model)\n",
    "out_of_fold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print()\n",
    "    train(model, optimizer, dataloader_train, epoch)\n",
    "    validate(model, df_train_org, df_val, dataloader_val, epoch, IDS_ALL_UNIQUE)\n",
    "    \n",
    "print(\"Final model 'pytorch_model.bin'\")\n",
    "\n",
    "torch.save(model.state_dict(), 'pytorch_model.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
